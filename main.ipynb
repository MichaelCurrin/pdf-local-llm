{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use AI to process a PDF\n",
    "> Convert PDF to text and process the text through an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama CLI demo\n",
    "\n",
    "Using Bash in the notebook without Python yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama run llama3.2 \"What is a PDF in 3 bullet points?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read PDF\n",
    "\n",
    "Load the PDF as text using Python. Pass a URL or a local path to a PDF.\n",
    "\n",
    "Here we read a Psychology PDF from [openstax.org](https://openstax.org/) as source of free PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_INPUT = \"https://assets.openstax.org/oscms-prodcms/media/documents/Psychology2e_WEB.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(PDF_INPUT)\n",
    "docs = loader.load_and_split()\n",
    "\n",
    "content = [d.page_content for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're splitting by page but this might not work out exactly, perhaps because of images.\n",
    "print(f\"chunks: {len(docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First line for each chunk.\n",
    "for chunk in content:\n",
    "    print(chunk.splitlines()[0])\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this demo we're using the Taste section at this chunk - at pg 177 of PDF, pg 164 of the book\n",
    "docs[194].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content[194])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate the PDF text\n",
    "\n",
    "Here converting from English to Dutch but you can use another prompt if you prefer like the sample below commented out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"llama3.2\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"dummy\")\n",
    "OPENAI_API_URL = os.getenv(\"OPENAI_API_URL\", \"http://localhost:11434/v1\")\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "    base_url=OPENAI_API_URL,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=OPENAI_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"\n",
    "You are expert at translating info from English to Dutch. Keep the original structure and with Markdown format.\n",
    "Give only the translated answer and nothing else, no explanation or preamble or conclusion.\"\n",
    "\n",
    "Context: '''{context}'''\n",
    "\"\"\".strip()\n",
    "\n",
    "# user_prompt = \"\"\"\n",
    "# Explain like I'm 5.\n",
    "\n",
    "# Explain the following in basic terms in bullet points. Give only the summarized content, with no preamble or conclusion.\n",
    "\n",
    "# Context: '''{context}'''\n",
    "# \"\"\".strip()\n",
    "\n",
    "\n",
    "template = ChatPromptTemplate(\n",
    "    [(\"system\", SYSTEM_PROMPT), (\"human\", user_prompt)]\n",
    ")\n",
    "chain = template | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the target chunk and a few more.\n",
    "sliced_content = content[194:194+3]\n",
    "\n",
    "for i, chunk in enumerate(sliced_content, start=1):\n",
    "    result = chain.invoke({\"context\": chunk})\n",
    "\n",
    "    print(i)\n",
    "    print(\"ORIGINAL\")\n",
    "    print(chunk)\n",
    "    print()\n",
    "    print(\"TRANSLATED\")\n",
    "    print(result.content)\n",
    "    print()\n",
    "    print('='*80)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
